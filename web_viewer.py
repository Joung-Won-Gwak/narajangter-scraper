"""
ë‚˜ë¼ì¥í„° ì…ì°°ê³µê³  ë·°ì–´ ì›¹ ì„œë²„
Flaskë¥¼ ì‚¬ìš©í•˜ì—¬ ìˆ˜ì§‘ëœ ê³µê³  ë°ì´í„°ë¥¼ ì›¹ í˜ì´ì§€ë¡œ í‘œì‹œí•©ë‹ˆë‹¤.
"""

import os
from flask import Flask, render_template, jsonify, request
import psycopg2
from psycopg2.extras import RealDictCursor
from dotenv import load_dotenv

load_dotenv()

app = Flask(__name__, template_folder='templates', static_folder='static')

def init_db():
    """ë°ì´í„°ë² ì´ìŠ¤ í…Œì´ë¸” ì´ˆê¸°í™”"""
    try:
        conn = get_db_connection()
        cur = conn.cursor()
        
        # ê¸°ì¡´ í…Œì´ë¸” ì‚­ì œ (ìŠ¤í‚¤ë§ˆ ë³€ê²½ ì ìš©ì„ ìœ„í•´)
        print("ğŸ—‘ï¸  ê¸°ì¡´ í…Œì´ë¸” ì‚­ì œ ì¤‘...")
        cur.execute("DROP TABLE IF EXISTS audit_notices;")
        conn.commit()
        print("âœ… ê¸°ì¡´ í…Œì´ë¸” ì‚­ì œ ì™„ë£Œ")
        
        # ìƒˆ í…Œì´ë¸” ìƒì„±
        create_table_sql = """
        CREATE TABLE IF NOT EXISTS audit_notices (
            id SERIAL PRIMARY KEY,
            notice_id VARCHAR(100) UNIQUE,
            title VARCHAR(500) NOT NULL,
            organization VARCHAR(200),
            publish_date DATE,
            deadline_date TIMESTAMP,
            estimated_price BIGINT,
            contract_method VARCHAR(100),
            notice_url TEXT,
            detail_content TEXT,
            raw_data JSONB,
            scraped_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        );
        """
        
        cur.execute(create_table_sql)
        conn.commit()
        cur.close()
        conn.close()
        print("âœ… ë°ì´í„°ë² ì´ìŠ¤ í…Œì´ë¸” ì´ˆê¸°í™” ì™„ë£Œ")
    except Exception as e:
        print(f"âŒ í…Œì´ë¸” ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

def get_db_connection():
    """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° - Railway DATABASE_URL ë˜ëŠ” ê°œë³„ ë³€ìˆ˜ ì§€ì›"""
    database_url = os.getenv("DATABASE_URL")
    
    if database_url:
        # DATABASE_URL íŒŒì‹±í•˜ì—¬ ê°œë³„ íŒŒë¼ë¯¸í„°ë¡œ ì „ë‹¬
        from urllib.parse import urlparse
        parsed = urlparse(database_url)
        db_name = parsed.path[1:]  # ì•ì˜ '/' ì œê±°
        print(f"ğŸ”— DB ì—°ê²°: host={parsed.hostname}, database={db_name}")
        return psycopg2.connect(
            host=parsed.hostname,
            port=parsed.port or 5432,
            database=db_name,
            user=parsed.username,
            password=parsed.password,
            cursor_factory=RealDictCursor
        )
    else:
        # ê°œë³„ í™˜ê²½ë³€ìˆ˜ ì‚¬ìš© (ë¡œì»¬ ê°œë°œìš©)
        db_name = os.getenv("POSTGRES_DB", "railway")
        print(f"ğŸ”— DB ì—°ê²° (ë¡œì»¬): database={db_name}")
        return psycopg2.connect(
            host=os.getenv("POSTGRES_HOST", "localhost"),
            port=int(os.getenv("POSTGRES_PORT", 5432)),
            database=db_name,
            user=os.getenv("POSTGRES_USER", "postgres"),
            password=os.getenv("POSTGRES_PASSWORD", "postgres"),
            cursor_factory=RealDictCursor
        )

# ì•± ì‹œì‘ ì‹œ í…Œì´ë¸” ì´ˆê¸°í™” (ì²« ìš”ì²­ ì „)
with app.app_context():
    init_db()

@app.route('/')
def index():
    """ë©”ì¸ í˜ì´ì§€"""
    return render_template('index.html')

@app.route('/api/notices')
def get_notices():
    """ê³µê³  ëª©ë¡ API"""
    try:
        conn = get_db_connection()
        cur = conn.cursor()
        
        # ê²€ìƒ‰ íŒŒë¼ë¯¸í„°
        keyword = request.args.get('keyword', '')
        
        query = """
            SELECT 
                notice_id,
                title,
                organization,
                publish_date,
                deadline_date,
                estimated_price,
                contract_method,
                notice_url,
                scraped_at
            FROM audit_notices
            WHERE 1=1
        """
        params = []
        
        if keyword:
            query += " AND title ILIKE %s"
            params.append(f"%{keyword}%")
        
        query += " ORDER BY publish_date DESC, scraped_at DESC"
        
        cur.execute(query, params)
        notices = cur.fetchall()
        
        # ë‚ ì§œ ë° ê¸ˆì•¡ í¬ë§·íŒ…
        result = []
        for notice in notices:
            item = dict(notice)
            if item['publish_date']:
                item['publish_date'] = str(item['publish_date'])
            if item['deadline_date']:
                item['deadline_date'] = str(item['deadline_date'])
            if item['scraped_at']:
                item['scraped_at'] = str(item['scraped_at'])
            if item['estimated_price']:
                item['estimated_price_formatted'] = f"{item['estimated_price']:,}ì›"
            else:
                item['estimated_price_formatted'] = '-'
            result.append(item)
        
        cur.close()
        conn.close()
        
        return jsonify({
            "success": True,
            "count": len(result),
            "data": result
        })
        
    except Exception as e:
        return jsonify({
            "success": False,
            "error": str(e),
            "data": []
        }), 500

@app.route('/api/stats')
def get_stats():
    """í†µê³„ API"""
    try:
        conn = get_db_connection()
        cur = conn.cursor()
        
        cur.execute("SELECT COUNT(*) as total FROM audit_notices")
        total = cur.fetchone()['total']
        
        cur.execute("""
            SELECT organization, COUNT(*) as count 
            FROM audit_notices 
            GROUP BY organization 
            ORDER BY count DESC 
            LIMIT 5
        """)
        top_orgs = cur.fetchall()
        
        cur.close()
        conn.close()
        
        return jsonify({
            "success": True,
            "total_notices": total,
            "top_organizations": [dict(o) for o in top_orgs]
        })
        
    except Exception as e:
        return jsonify({"success": False, "error": str(e)}), 500

@app.route('/api/scrape', methods=['POST'])
def run_scraper():
    """ê³µê³µë°ì´í„°í¬í„¸ì—ì„œ ë°ì´í„° ìˆ˜ì§‘ ì‹¤í–‰"""
    try:
        import sys
        import traceback
        from openapi_scraper import NarajangterPipeline
        
        pipeline = NarajangterPipeline()
        result = pipeline.run(max_pages=5)
        
        return jsonify({
            "success": result["success"],
            "scraped_count": result["scraped_count"],
            "inserted_count": result["inserted_count"],
            "errors": result["errors"][:5] if result["errors"] else []
        })
        
    except Exception as e:
        # ìƒì„¸ ì—ëŸ¬ ë¡œê·¸ ì¶œë ¥
        traceback.print_exc()
        return jsonify({
            "success": False,
            "error": str(e),
            "traceback": traceback.format_exc(),
            "scraped_count": 0,
            "inserted_count": 0
        }), 500


if __name__ == '__main__':
    init_db()  # í…Œì´ë¸” ì´ˆê¸°í™”
    app.run(debug=True, host='0.0.0.0', port=5000)

